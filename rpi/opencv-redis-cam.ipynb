{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LFOs\n",
    "could generate a LFO that can be applied to whatever image transformation function.\n",
    "You would generate values within a certain (configurable range), and then they would be clamped down into the \n",
    "appropriate parameter.\n",
    "There is probably a library for creating LFO values, would be useful for the different shapes, for example. These are wave forms. \n",
    "\n",
    "* frequence\n",
    "* depth\n",
    "* shape\n",
    "\n",
    "So for example,  blur = cv2.blur(img, (____, ______)) Would get filled in with the lfo value squashed appropriately.\n",
    "* height/width of space selection\n",
    "* x/y of space selection (effecting spatial selection would effect the transformation)\n",
    "* R/G/B values\n",
    "\n",
    "We would like to be able to send it to multiple parameters simultaneously; not sure how the interface would look. Perhaps the boolean algebra would help (&&)\n",
    "https://docs.scipy.org/doc/scipy-0.14.0/reference/generated/scipy.signal.sawtooth.html\n",
    "### Transformations\n",
    "#### Smoothing\n",
    " https://stackoverflow.com/questions/37409811/smoothing-edges-of-a-binary-image\n",
    " blur = cv2.GaussianBlur(img,(5,5),0)\n",
    "smooth = cv2.addWeighted(blur,1.5,img,-0.5,0)\n",
    " -  blur = cv2.blur(img, (100, 100))\n",
    " - 'curve straight lines' / 'smooth corners' / 'twist image'\n",
    " - project a selection elsewhere  \n",
    " https://stackoverflow.com/questions/19068085/shift-image-content-with-opencv\n",
    " https://stackoverflow.com/questions/23464495/fastest-way-to-move-image-in-opencv\n",
    "### Selections\n",
    " - should be able to represent every selection as a 'mask' array of some kind\n",
    " https://stackoverflow.com/questions/36911877/cropping-circle-from-image-using-opencv-python\n",
    " \n",
    "### Other Stuff\n",
    "https://www.arduino.cc/en/Tutorial/Smoothing\n",
    "https://opencv-python-tutroals.readthedocs.io/en/latest/py_tutorials/py_imgproc/py_filtering/py_filtering.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TODO [#B] Video Synth Project with Mark\n",
    "Screen capture \n",
    "How do I get video input outside of the camera?\n",
    "Composite video splitter’d be nice\n",
    "Composite video stream via usb\n",
    "Capture the camera input stream; want to treat this equivalent to any other video stream, such as from USB from video synths\n",
    "\n",
    "Then capture this video stream and manipulate with ffmpeg-python or moviepy\n",
    " - store a clip \n",
    " - displace image: this is an instance of ‘picture-in-picture-witout-backdrop w/ configurable size’; actually always want ‘some backdrop’\n",
    "### PinP\n",
    " - [ ] x/y knobs for size (height & width)\n",
    " - [ ] joystick for movement\n",
    " - [ ] toggle background: should be ‘transparent’ but I don’t think ‘transparent’ exists in the way I want it to\n",
    " - size is resolution\n",
    " - movement is . . . . \n",
    "\n",
    "### Programming Install Jupyter-notebook on rpi probably. or use mousiecam\n",
    "### Make RPi ready\n",
    "   - re-enable internet\n",
    "   - install python packages: nltk, spacy, nltk.download, spacy.download, moviepy, ffmpeg, pygame\n",
    "   - install system level: ffmpeg, etc.\n",
    "### Image Rotation   clip.rotate\n",
    "\n",
    "### Colorize ‘outline’ where white becomes dark, for example.\n",
    "\n",
    "### Links\n",
    "   - http://hannes.enjoys.it/blog/2016/03/ffmpeg-on-raspbian-raspberry-pi/\n",
    "   - https://www.raspberrypi.org/forums/viewtopic.php?t=238826\n",
    "   - https://www.ebay.com/p/2164324534\n",
    "   - https://github.com/Apress/raspberry-pi-gpu-audio-video-prog/blob/master/OpenGLES/Hello_Square.c\n",
    "   - https://febon.blogspot.com/2012/02/1.html\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import redis\n",
    "import numpy as np\n",
    "import cv2\n",
    "import json\n",
    "fp = '/Users/averagehat/Dropbox/video-synth/rpi/opencv/samples/data/opencv-logo.png'\n",
    "# from matplotlib import pyplot as plt\n",
    "faceCascade = cv2.CascadeClassifier(\"haarcascade_frontalface_default.xml\")\n",
    "start_img = cv2.imread(fp, cv2.IMREAD_COLOR)\n",
    "\n",
    "x=256\n",
    "y=256\n",
    "r=100\n",
    "# crop image as a square\n",
    "img = start_img[y:y+r*2, x:x+r*2]\n",
    "# create a mask\n",
    "mask = np.full((img.shape[0], img.shape[1]), 0, dtype=np.uint8) \n",
    "# create circle mask, center, radius, fill color, size of the border\n",
    "cv2.circle(mask,(r,r), r, (255,255,255),-1)\n",
    "# get only the inside pixels\n",
    "fg = cv2.bitwise_or(img, img, mask=mask)\n",
    "\n",
    "mask = cv2.bitwise_not(mask)\n",
    "background = np.full(img.shape, 255, dtype=np.uint8)\n",
    "bk = cv2.bitwise_or(background, background, mask=mask)\n",
    "final = cv2.bitwise_or(fg, bk)\n",
    "cv2.imshow('image',final)\n",
    "cv2.waitKey(0)\n",
    "cv2.destroyAllWindows()\n",
    "# plt.subplot(122),plt.imshow(final),plt.title('Final')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fp = '/Users/averagehat/Dropbox/video-synth/rpi/opencv/samples/data/opencv-logo.png'\n",
    "import cv2\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "# Suppose center of the circle is (a1,b1) and radius is r. Then cropping coordinates would be [ a1-r:a1+r , b1-r:b1+r ].\n",
    "\n",
    "y=256\n",
    "x=256\n",
    "r=63\n",
    "img = cv2.imread(fp)\n",
    "cv2.circle(img,(x,y), r, (0,0,0),2)\n",
    "result = img[y-r:y+r,x-r:x+r]\n",
    "blur = cv2.blur(img, (100, 100))\n",
    "plt.subplot(121),plt.imshow(img),plt.title('Original')\n",
    "plt.subplot(122),plt.imshow(blur),plt.title('Blurred')\n",
    "plt.subplot(122),plt.imshow(result),plt.title('Blurred')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#cap = cv2.VideoCapture('moviepy/media/video_with_failing_audio.mp4')\n",
    "cap = cv2.VideoCapture(0)\n",
    "cap.set(cv2.CAP_PROP_FOURCC,cv2.VideoWriter_fourcc('M','J','P','G'))\n",
    "fourcc = cv2.VideoWriter_fourcc(*'XVID')\n",
    "\n",
    "# out = cv2.VideoWriter('output.avi',fourcc, 20.0, (640,480))\n",
    "RECORDING = False\n",
    "VIDNUMBER = 0\n",
    "GRAY = False\n",
    "outname = f'output.avi'\n",
    "r = redis.Redis(host='localhost', port=7777, db=0)\n",
    "listener = r.pubsub()\n",
    "listener.subscribe('rpi-input')\n",
    "def scaleBetween(unscaledNum, minAllowed, maxAllowed, min, max):\n",
    "  return (maxAllowed - minAllowed) * (unscaledNum - min) / (max - min) + minAllowed;\n",
    "\n",
    "settings = {\n",
    "    'grayscale' : False,\n",
    "    'text' : None,\n",
    "    'meter': 0.0,\n",
    "    'mirror': False,\n",
    "    'eval' : None\n",
    "}        \n",
    "def get_new_settings(old, raw_data):\n",
    "    new = json.loads(raw_data)\n",
    "    return {\n",
    "        'grayscale' : (not old['grayscale']) if new.get('graytoggle') else old['grayscale'],\n",
    "        'mirror' : (not old['mirror']) if new.get('flip') else old['mirror'],\n",
    "        'text' : new.get('text', old['text']),\n",
    "        'meter' : new.get('meter', old['meter']),\n",
    "        'eval' : new.get('eval'),\n",
    "        'x': new.get('x', old.get('x')),\n",
    "        'y' : new.get('y', old.get('y'))\n",
    "    }\n",
    "# cv2.circle(img, center, radius, color, thickness=1, lineType=8, shift=0)\n",
    "def apply_effects(settings, frame):\n",
    "    if settings['grayscale']:\n",
    "        frame = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n",
    "    if settings['mirror']:\n",
    "        frame = cv2.flip(frame, 1)\n",
    "    if settings.get('x') or settings.get('y'):\n",
    "        x, y = settings.get('x', 240), settings.get('y', 240)\n",
    "        scaled_x = int(scaleBetween(x, 30, 400, 0, 1050))\n",
    "        scaled_y = int(scaleBetween(y, 30, 400, 0, 1050))\n",
    "        print(f'scaled: {scaled_x}, {scaled_y}')\n",
    "        cv2.circle(frame, (scaled_x, scaled_y), 20, (255, 0, 0), 2)\n",
    "    if settings['text']:\n",
    "        # frame = cv2.text(text) \n",
    "        gray = cv2.cvtColor(raw_frame, cv2.COLOR_BGR2GRAY)\n",
    "        faces = faceCascade.detectMultiScale(\n",
    "            gray,\n",
    "            scaleFactor=1.1,\n",
    "            minNeighbors=5,\n",
    "            minSize=(30, 30),\n",
    "            flags=cv2.CASCADE_SCALE_IMAGE\n",
    "        )\n",
    "    \n",
    "        # Draw a rectangle around the faces\n",
    "        for (x, y, w, h) in faces:\n",
    "            cv2.rectangle(frame, (x, y), (x+w, y+h), (0, 255, 0), 2)\n",
    "    if settings['eval']:\n",
    "        frame = eval(settings['eval'])\n",
    "    return frame\n",
    "        \n",
    "#def run_cam(thread_print): \n",
    "if True:\n",
    "    while(cap.isOpened()):\n",
    "    \n",
    "        ret, raw_frame = cap.read()\n",
    "        key = cv2.waitKey(1)\n",
    "        if key & 0xFF == ord('q'): break\n",
    "            \n",
    "        if ret:\n",
    "            msg = listener.get_message()\n",
    "            if msg: print(msg)\n",
    "            if msg and msg.get('type') == 'message':\n",
    "                data, channel, msg_type = msg['data'], msg['channel'], msg['type']\n",
    "                \n",
    "                settings = get_new_settings(settings, data)\n",
    "                print(f'New Settings: {settings}')\n",
    "            frame = apply_effects(settings, raw_frame)\n",
    "            cv2.imshow(\"Image\", frame)\n",
    "        else:\n",
    "           print('no video')\n",
    "           cap.set(cv2.CAP_PROP_POS_FRAMES, 0)\n",
    "    cap.release()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(raw_frame[0,:])\n",
    "# Recording code\n",
    "\n",
    "            #if RECORDING:\n",
    "            #    out.write(frame)\n",
    "\n",
    "    \n",
    "   #     elif key & 0xFF == ord('r'):\n",
    "           # cap.set(cv2.CAP_PROP_POS_FRAMES, 0)\n",
    "            #RECORDING = not RECORDING\n",
    "            #if RECORDING:\n",
    "            #    VIDNUMBER += 1\n",
    "            #    outname = f'output-{VIDNUMBER}.avi'\n",
    "            #    out = cv2.VideoWriter(outname,fourcc, 20.0, (640,480))\n",
    "            #else:\n",
    "            #    out.release() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# https://www.geeksforgeeks.org/circle-detection-using-opencv-python/\n",
    "# f'= cv2.circle{(\"frame\", center_coordinates, radius, color, thickness_in_pixels) }'\n",
    "   cv2.circle('frame', (120, 50), 20, (255, 0, 0), 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nbmultitask import ThreadWithLogAndControls"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "type: One of the following: 'subscribe', 'unsubscribe', 'psubscribe', 'punsubscribe', 'message', 'pmessage'\n",
    "channel: The channel [un]subscribed to or the channel a message was published to\n",
    "pattern: The pattern that matched a published message's channel. Will be None in all cases except for 'pmessage' types.\n",
    "data: The message data. With [un]subscribe messages, this value will be the number of channels and patterns the connection is currently subscribed to. With [p]message messages, this value will be the actual published message."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "settings['grayscale'] = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://stackoverflow.com/questions/18064914/how-to-use-opencv-python-to-blur-faces"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image = cv2.imread(imagepath)\n",
    "result_image = image.copy()\n",
    "\n",
    "# Specify the trained cascade classifier\n",
    "\n",
    "\n",
    "\n",
    "# Create a cascade classifier\n",
    "face_cascade.load(face_cascade_name)\n",
    "\n",
    "#Preprocess the image\n",
    "grayimg = cv2.cvtColor(image, cv2.cv.CV_BGR2GRAY)\n",
    "grayimg = cv2.equalizeHist(grayimg)\n",
    "\n",
    "#Run the classifiers\n",
    "faces = face_cascade.detectMultiScale(grayimg, 1.1, 2, 0|cv2.cv.CV_HAAR_SCALE_IMAGE, (30, 30))\n",
    "\n",
    "print \"Faces detected\"\n",
    "\n",
    "if len(faces) != 0:         # If there are faces in the images\n",
    "    for f in faces:         # For each face in the image\n",
    "    img[startX:endX, startY:endY] = cv2.blur(img[startX:endX, startY:endY], (23, 23))\n",
    "\n",
    "        # Get the origin co-ordinates and the length and width till where the face extends\n",
    "        x, y, w, h = f\n",
    "\n",
    "        # get the rectangle img around all the faces\n",
    "        cv2.rectangle(image, (x,y), (x+w,y+h), (255,255,0), 5)\n",
    "        sub_face = image[y:y+h, x:x+w]\n",
    "        # apply a gaussian blur on this new recangle image\n",
    "        sub_face = cv2.GaussianBlur(sub_face,(23, 23), 30)\n",
    "        # merge this blurry rectangle to our final image\n",
    "        result_image[y:y+sub_face.shape[0], x:x+sub_face.shape[1]] = sub_face\n",
    "        face_file_name = \"./face_\" + str(y) + \".jpg\"\n",
    "        cv2.imwrite(face_file_name, sub_face)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
